import streamlit as st
import speech_recognition as sr
import numpy as np
import tempfile
import os
from pydub import AudioSegment
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import queue
import time
import threading
import io
import logging
from typing import Optional, List
import whisper  # OpenAI Whisperãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¥ãƒ¼ã¨ãƒ•ãƒ©ã‚°
audio_frames_queue = queue.Queue()
recording_active = threading.Event()
audio_data_buffer = []

# ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
logger = logging.getLogger(__name__)

# WhisperéŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã§1å›ã ã‘èª­ã¿è¾¼ã‚€ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šï¼‰
# model_sizeã¯base, small, medium, largeã‹ã‚‰é¸æŠå¯èƒ½ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
@st.cache_resource
def load_whisper_model():
    return whisper.load_model("base")

def record_audio_webrtc():
    """
    WebRTCã‚’ä½¿ç”¨ã—ãŸãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ã®éŸ³å£°éŒ²éŸ³æ©Ÿèƒ½
    """
    # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ãƒˆã‚’ç¢ºèª
    if "recording_state" not in st.session_state:
        st.session_state.recording_state = "inactive"
        st.session_state.audio_data = None
        st.session_state.recognized_text = None
    
    # Whisperãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ã®ã§åŠ¹ç‡çš„ï¼‰
    whisper_model = load_whisper_model()
    
    # UIç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
    status_placeholder = st.empty()
    button_col1, button_col2 = st.columns(2)
    text_placeholder = st.empty()
    
    # éŒ²éŸ³å®Œäº†å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã‚¨ãƒªã‚¢
    if st.session_state.recognized_text:
        text_placeholder.success(f"èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {st.session_state.recognized_text}")
    
    # éŒ²éŸ³çŠ¶æ…‹ã«å¿œã˜ãŸUIã®è¡¨ç¤º
    if st.session_state.recording_state == "inactive":
        status_placeholder.info("ğŸ¤ ã€ŒéŒ²éŸ³é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€è‹±èªã§è©±ã—ã¦ãã ã•ã„")
    elif st.session_state.recording_state == "recording":
        status_placeholder.warning("ğŸ”´ éŒ²éŸ³ä¸­... è‹±èªã§è©±ã—ã¦ãã ã•ã„")
    elif st.session_state.recording_state == "processing":
        status_placeholder.info("â³ éŸ³å£°ã‚’å‡¦ç†ä¸­...")
    elif st.session_state.recording_state == "completed":
        if st.session_state.recognized_text:
            status_placeholder.success("âœ… éŸ³å£°èªè­˜å®Œäº†ï¼")
        else:
            status_placeholder.error("âŒ éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚")
    
    # éŒ²éŸ³é–‹å§‹ãƒœã‚¿ãƒ³
    if button_col1.button("ğŸ¤ éŒ²éŸ³é–‹å§‹", type="primary", disabled=st.session_state.recording_state == "recording"):
        audio_data_buffer.clear()
        st.session_state.recording_state = "recording"
        recording_active.set()
        st.rerun()
    
    # éŒ²éŸ³åœæ­¢ãƒœã‚¿ãƒ³
    if button_col2.button("â¹ï¸ éŒ²éŸ³åœæ­¢", disabled=st.session_state.recording_state != "recording"):
        recording_active.clear()
        st.session_state.recording_state = "processing"
        
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å–å¾—
        if audio_data_buffer:
            # ãƒãƒƒãƒ•ã‚¡ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            audio_array = np.concatenate(audio_data_buffer)
            
            # éŸ³å£°èªè­˜å‡¦ç†
            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                    temp_filename = f.name
                
                # pydubã‚’ä½¿ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                # sample_widthã‚’2ï¼ˆ16ãƒ“ãƒƒãƒˆï¼‰ã«è¨­å®šã€ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’16000ã«è¨­å®šï¼ˆWhisperã®æ¨å¥¨ãƒ¬ãƒ¼ãƒˆï¼‰
                audio_segment = AudioSegment(
                    audio_array.tobytes(), 
                    frame_rate=16000,  # ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆã‚’Whisperæ¨å¥¨ã®16kHzã«å¤‰æ›´
                    sample_width=2,
                    channels=1
                )
                
                # éŸ³é‡ã‚’å¢—å¹…ã—ã¦èªè­˜ç²¾åº¦ã‚’å‘ä¸Š
                audio_segment = audio_segment + 10  # 10dBå¢—å¹…
                
                # ãƒã‚¤ã‚ºãƒªãƒ€ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆç„¡éŸ³éƒ¨åˆ†ã®é™¤å»ï¼‰
                audio_segment = audio_segment.strip_silence(
                    silence_len=500,  # 500msä»¥ä¸Šã®ç„¡éŸ³ã‚’ç„¡è¦–
                    silence_thresh=-40,  # -40dBä»¥ä¸‹ã‚’ç„¡éŸ³ã¨åˆ¤æ–­
                    padding=300  # å‰å¾Œã«300msã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
                )
                
                audio_segment.export(temp_filename, format="wav")
                
                # Whisperãƒ¢ãƒ‡ãƒ«ã§éŸ³å£°èªè­˜
                with st.spinner("Whisperã§éŸ³å£°ã‚’åˆ†æä¸­..."):
                    result = whisper_model.transcribe(
                        temp_filename,
                        language="en",  # è‹±èªã¨ã—ã¦èªè­˜
                        fp16=False,  # ç²¾åº¦ã‚’å„ªå…ˆ
                        temperature=0.0  # ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æœ€å°åŒ–
                    )
                    recognized_text = result["text"].strip()
                    
                    if recognized_text:
                        st.session_state.recognized_text = recognized_text
                    else:
                        st.session_state.recognized_text = None
                        raise Exception("éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†å°‘ã—å¤§ããªå£°ã§ã¯ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„ã€‚")
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                os.unlink(temp_filename)
                
                st.session_state.recording_state = "completed"
                
            except Exception as e:
                logger.error(f"éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: {e}")
                st.session_state.recording_state = "completed"
                st.session_state.recognized_text = None
        else:
            st.session_state.recording_state = "inactive"
            
        st.rerun()
    
    # WebRTCã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ (éŸ³å£°ã®ã¿ãƒ¢ãƒ¼ãƒ‰) - ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¢—åŠ 
    webrtc_ctx = webrtc_streamer(
        key="speech-to-text",
        mode=WebRtcMode.SENDONLY,
        audio_receiver_size=32768,  # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¢—åŠ 
        media_stream_constraints={
            "video": False, 
            "audio": {
                "echoCancellation": True,  # ã‚¨ã‚³ãƒ¼ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                "noiseSuppression": True,  # ãƒã‚¤ã‚ºæŠ‘åˆ¶
                "autoGainControl": True    # è‡ªå‹•ã‚²ã‚¤ãƒ³åˆ¶å¾¡
            }
        },
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    )
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    if webrtc_ctx.audio_receiver and recording_active.is_set():
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
            audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
            for audio_frame in audio_frames:
                sound_data = audio_frame.to_ndarray()
                audio_data_buffer.append(sound_data)
                
        except queue.Empty:
            pass
    
    # ç¾åœ¨ã®çŠ¶æ…‹ã«å¿œã˜ã¦çµæœã‚’è¿”ã™
    if st.session_state.recording_state == "completed" and st.session_state.recognized_text:
        return st.session_state.recognized_text
    return None

def record_audio_text():
    """
    ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    æˆ»ã‚Šå€¤: å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯ Noneï¼ˆã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ï¼‰
    """
    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨
    error_placeholder = st.empty()
    
    # å…¥åŠ›çŠ¶æ…‹è¡¨ç¤º
    status_placeholder = st.empty()
    status_placeholder.info("ğŸ–Šï¸ è‹±èªã§å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
        user_input = st.text_input("è‹±èªã§å›ç­”ã‚’å…¥åŠ›:", key="voice_replacement_input")
        
        if st.button("å›ç­”ã‚’é€ä¿¡", key="submit_text_answer"):
            if user_input:
                # å…¥åŠ›å®Œäº†è¡¨ç¤º
                status_placeholder.success("âœ“ å›ç­”ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸï¼")
                return user_input
            else:
                error_placeholder.error("ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return None
            
    except Exception as e:
        error_placeholder.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # å…¥åŠ›ãŒãªã„å ´åˆã¯Noneã‚’è¿”ã™
    return None

def record_audio():
    """
    éŸ³å£°å…¥åŠ›ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’æä¾›ã™ã‚‹é–¢æ•°
    æˆ»ã‚Šå€¤: èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯ None
    """
    # ã‚¿ãƒ–ã‚’ä½œæˆã—ã¦éŸ³å£°å…¥åŠ›ã¨ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã«
    tab1, tab2 = st.tabs(["ğŸ¤ éŸ³å£°ã§å›ç­”", "âŒ¨ï¸ ãƒ†ã‚­ã‚¹ãƒˆã§å›ç­”"])
    
    with tab1:
        result = record_audio_webrtc()
        if result:
            return result
    
    with tab2:
        result = record_audio_text()
        if result:
            return result
    
    return None

def voice_input_button():
    """
    å›ç­”å…¥åŠ›ãƒœã‚¿ãƒ³ã¨ãã®å‡¦ç†ã‚’æä¾›ã™ã‚‹é–¢æ•°
    æˆ»ã‚Šå€¤: å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯ Noneï¼ˆæœªå…¥åŠ›æ™‚ã‚„ã‚¨ãƒ©ãƒ¼æ™‚ï¼‰
    """
    if st.button("ğŸ¤ å›ç­”ã™ã‚‹", type="primary", key="voice_input_button"):
        # éŸ³å£°ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’å®Ÿè¡Œ
        recognized_text = record_audio()
        
        # å…¥åŠ›çµæœã‚’è¿”ã™
        return recognized_text
    
    # ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã„ãªã„å ´åˆã¯Noneã‚’è¿”ã™
    return None 